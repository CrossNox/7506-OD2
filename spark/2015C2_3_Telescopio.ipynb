{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un telescopio registra automaticamente la luminosidad de distintas\n",
    "estrellas generando un RDD con registros de tipo (star_id,\n",
    "magnitude,spectral_type, timestamp). Queremos obtener un listado de\n",
    "estrellas que tienen el mismo tipo espectral e igual promedio de\n",
    "magnitud una vez redondeado el mismo a un decimal. El resultado\n",
    "debe ser una lista en donde cada elemento de la lista es una lista de ids de estrellas similares. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De [aca](http://www.astronexus.com/hyg) saco un csv de estrellas. Lo cargo como DataFrame para leerlo mas facil. Preproceso el DF y luego obtengo el RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "faker = Faker()\n",
    "\n",
    "df = spark.read.csv(\"textos/hygfull.csv.gz\",header=True).select([\"StarId\",\"Mag\",\"Spectrum\"])\n",
    "udf_timestamp = udf(lambda x: faker.unix_time(),IntegerType())\n",
    "udf_spectrum = udf(lambda x: x.strip()[0] if len(x.strip())>0 else \"\",StringType())\n",
    "df = df.withColumn(\"timestamp\",udf_timestamp(\"StarId\"))\n",
    "df = df.withColumn(\"Spectrum\",udf_spectrum(\"Spectrum\"))\n",
    "df = df.withColumn(\"Mag\",df[\"Mag\"].cast(DoubleType()))\n",
    "df = df.withColumn(\"StarId\",df[\"StarId\"].cast(IntegerType()))\n",
    "df = df.filter(df.Spectrum != \"\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df.rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos agrupar por tipo espectral (tercer columna) y promedio de magnitud (segunda columna). Nos piden que el promedio de magnitud este redondeado a un decimal. Creamos la clave para agrupar y como valor el id. Agrupamos por clave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda x: ((round(x[1],1),x[2]),x[0])).groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos algo del tipo <clave, iterable de StarIds>. Queremos convertir ese iterable a una lista. Una opcion seria hacer `.map(lambda x: (x[0], list(x[1])))`, pero es mucho mas prolijo hacerlo usando [.mapValues](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=mapvalues#pyspark.RDD.mapValues):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.mapValues(list).map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que dice el enunciado, no importa la clave, solo la lista de similares. Entonces en la celda anterior nos quedamos con solo la lista de similares de cada registro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
